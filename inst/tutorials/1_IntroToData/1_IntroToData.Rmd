---
title: "Topic 1: An Introduction to Data and Sampling"
tutorial:
  id: "IntroData.Topic1.AppliedStats"
  version: 1.0
output: 
  learnr::tutorial:
    progressive: TRUE
    
runtime: shiny_prerendered
---

```{r global-options, include=FALSE}
#knitr::opts_chunk$set(eval = FALSE)
library(dplyr)
library(ggplot2)
library(learnr)
library(gradethis)
library(openintro)

set.seed(42)

houses <- read.csv("https://raw.githubusercontent.com/NikhilKumarMutyala/Linear-Regression-from-scartch-on-KC-House-Dataset/master/kc_house_data.csv")
houses$exterior <- sample(c("brick", "vinyl siding", "shingles", "NA"), size = nrow(houses), prob = c(0.15, 0.7, 0.12, 0.03), replace = TRUE)
houses$month <- sample(1:12, size = nrow(houses), replace = TRUE)
housesSubset <- houses[ , c(1, 23, 3, 6, 7, 9, 22)]

#knitr::opts_chunk$set(exercise.checker = gradethis::grade_learnr)
```

## Data and Sampling

This is the first of a series of interactive workbooks you'll use to engage content from introductory statistics. This workbook covers an introduction to data, including some background on experimental design and data collection.

## An Introduction to Data

###

In this workbook you'll encounter two datasets -- one concerning real estate from King County, WA (`houses`) and another containing prices and attributes of almost 54,000 diamonds (`diamonds`). A few rows of the `houses` data is shown below in a convenient form where the rows of data represent *records* (sometimes called **observations**), and the columns represent **variables** (sometimes called **features**). We'll see later in our course that such data is called *tidy*.

###

```{r showData, exercise = FALSE, echo = FALSE}
head(housesSubset[sample(1:nrow(housesSubset), size = nrow(housesSubset), replace = FALSE), ])
```

You can see the first six rows of the dataset above. A simplified *data dictionary* (a map of column names and explanations) appears below.

###

+ `id` provides the property identification number.
+ `month` gives the month that the property was listed for sale.
+ `price` is the listing price of the property in US dollars (\$).
+ `sqft_living` gives the finished square footage of the home.
+ `sqft_lot` gives the square footage of the property (land).
+ `waterfront` indicates whether the property is waterfront.
+ `exterior` provides a description of the exterior covering of the home.


## Variable Types

###

Variables which take on numerical values <u>and</u> for which measures such as the mean, median, or standard deviation are meaningful are referred to as **numerical**. Variables which serve to group data into categories are called **categorical** --  examples include the color of a car or an area code prefix for a telephone number. Note that a telephone area code looks numeric, but since taking their average is meaningless, we consider it to be categorical. It is possible for data to be neither numerical nor categorical -- for example, a **unique identifier** is a non-numeric variable which is unique to each record. A timestamp may be an example of a unique identifier -- all is not lost, however, with a bit of pre-processing we can sometimes extract useful information from these columns.

###

Answer the following using your knowledge of the dataset and variable types.

```{r id-vars-numerical, echo = FALSE}
quiz(
question_checkbox(
"Check off each of the variables from the `houses` dataset which are numerical",
answer("id", message = "are we able to take a meaningful average of these values?"),
answer("month", message = "is an average meaningful here?"),
answer("price", correct = TRUE),
answer("sqft_living", correct = TRUE),
answer("sqft_lot", correct = TRUE),
answer("waterfront"),
answer("exterior"),
allow_retry = TRUE,
try_again = "Be sure to select all of them! There are three."
),

question_checkbox(
"Check off each of the variables from the `houses` dataset which are categorical",
answer("id", message = "are we able to take a meaningful average of these values?"),
answer("month", correct = TRUE),
answer("price"),
answer("sqft_living"),
answer("sqft_lot"),
answer("waterfront", correct = TRUE),
answer("exterior", correct = TRUE),
allow_retry = TRUE,
try_again = "Be sure to select all of them!"
),

question_checkbox(
"Check off each of the variables from the `houses` dataset which are unique identifiers.",
answer("id", correct = TRUE),
answer("month"),
answer("price"),
answer("sqft_living"),
answer("sqft_lot"),
answer("waterfront"),
answer("exterior"),
allow_retry = TRUE,
try_again = "There's just one here!"
)

)
```

###

The **levels** of a variable are the different (unique) values that the variable takes on. For example, a dataset on students might include a variable called `ClassYear` with the levels *Freshman*, *Sophomore*, *Junior*, *Senior*. Numerical variables also have levels -- usually there are lots of levels corresponding to a numerical variable, but if there are too few, we may be better off considering the corresponding variable to be categorical. For example, if we had a dataset that included a `Year` variable, but the only observed levels in the dataset are 2008 and 2017, we may be better off thinking about `Year` as a categorical variable than as a numerical one.

## Relationships between variables

###

**Association, Independence, Correlation:** Two variables are *associated* with one another if a change in levels of one is generally accompanied by change in the other. That is, larger values of one variable are accompanied by larger (or smaller) values in the other. Think â€“ does knowing something about one of the variables give me any information about the other? If two variables are not associated, then we might say that they are *independent* of one another. Lastly, *correlation* is a way to formally measure the strength of a <u>LINEAR</u> association between two variables. Look at the plots considering characteristics of various diamonds below.

###

```{r plots-associations-num, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, results = FALSE, fig.width = 3, fig.height = 3}
ggplot(data=diamonds) + geom_point(mapping = aes(x = carat, y = price), color = "blue", alpha = .15) + labs(title = "Price by Carat", x = "Carat", y = "Price")
ggplot(data=diamonds) + geom_point(mapping = aes(x = carat, y = depth), color = "blue", alpha = .15) + labs(title = "Carat by Depth", x = "Depth", y = "Carat")
ggplot(data=diamonds) + geom_point(mapping = aes(x = x, y = y), color = "blue", alpha = .15) + labs(title = "Length by Width", x = "Length (mm)", y = "Width (mm)")
```

###

Use the plots above to answer the following questions.

```{r plots-associations-questions-num, echo = FALSE}
quiz(
question_checkbox(
"Which of the plots above highlight an association between the corresponding variables?",
answer("Carat versus Price", correct = TRUE),
answer("Depth versus Carat"),
answer("Length by Width", correct = TRUE),
allow_retry = TRUE
),

question(
"Which pair of variables from the above plots is independent?",
answer("price and carat"),
answer("carat and depth", correct = TRUE),
answer("length and width"),
allow_retry = TRUE
),

question(
"Which of the plots shows the strongest correlation between the corresponding variables?",
answer("Carat versus Price"),
answer("Depth versus Carat"),
answer("Length by Width", correct = TRUE),
allow_retry = TRUE
)

)
```

###

Since both of the variables in each of the plots above are numerical, we can describe the direction of the association. Notice that there is a *positive* association in both of the plots you identified above, since an increase in one of the variables is generally accompanied by an increase in the other. If two numerical variables are associated but an increase in one is generally accompanied by a decrease in the other, we say that the association is *negative*. For those familiar with lines and slopes, the direction of the association corresponds to the sign on the slope of a line of "best fit" (which we will discuss at the end of our course).

###

We can also identify whether an association exists between variables when one or more are categorical. Consider the plots below which refer back to our `houses` dataset from earlier.

```{r plots-association-cat, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, results = FALSE, fig.width = 4, fig.height = 4}
housesSubset$waterfront <- ifelse(housesSubset$waterfront == 1, "yes", "no")
housesSubset$waterfront <- as.factor(housesSubset$waterfront)
ggplot(data=housesSubset) + geom_boxplot(mapping = aes(x = waterfront, y = price)) + labs(title = "Price by Waterfront", x = "Waterfront", y = "Price", group = "Waterfront") + scale_y_log10()
ggplot(data=housesSubset) + geom_bar(mapping = aes(x = waterfront, fill = exterior), position = "fill") + labs(title = "Exterior Covering and Waterfront", x = "Waterfront") + theme(legend.position = "below")
```

###

```{r plots-associations-questions-cat, echo = FALSE}
question_checkbox(
"One of the plots above shows an association between the corresponding variables. Which one is it?",
answer("Price by Waterfront", correct = TRUE),
answer("Exterior Covering and Waterfront"),
allow_retry = TRUE
)
```

###

Did you get the previous question right on the first try? Think about why the answer is as this workbook indicates and ask a question if you need more clarification.

###

**Major Questions In Statistics**: Given groups with different characteristics (differing levels) regarding variable *X*, do they differ with respect to variable *Y*?

###

We'll find that we can't just answer these questions by looking at plots involving some sample data. Why not?

## Data collection principles

###

**Population vesus Sample**: In statistics, we almost always want to apply generalizations from a small sample to a large population -- you might think of this as a sort of *stereotyping*. The trick here is that for our assertions (generalizations) to be valid, our sample must be *representative of* our population.

###

<center> **The following takeaway is critical:** *Results based off of a sample may only be generalized to a population for which that sample is representative.* </center>
<br/>

###

**Sampling Strategies**: There are many sampling techniques. We will focus on census, simple random sample, stratified sample, and convenience sample. 

###

  * If we could sample every object in a population we would be taking a *census*. While census would give us certainty in an answer to a statistical question, it is infeasible to conduct a census due to non-response and fluid populations.

<center>
```{r echo = FALSE, eval = TRUE, fig.width=3.5, fig.height=2.75}
#set.seed(21)
pop <- data.frame(x = sample(1:100, 100, replace = TRUE), y = sample(1:100, 100, replace = TRUE))
ggplot(data = pop) + geom_point(mapping = aes(x = x, y = y), size = 2, color = "orange", alpha = 0.8) + labs(title = "Census", x = "Entire Population Sampled (orange)")
```
</center>

  <center> **Please Listen** to [this NPR story](http://www.npr.org/templates/story/story.php?storyId=125380052){target="_blank"}  which aired prior to the 2010 Census
</center>
<br/>

###

  * The *simple random sample* is the GOLD STANDARD in statistics. We randomly select some "large enough" number of individuals from the entire population and record variables from them. The advantage here is that we are likely to attain a sample of results that are representative of the entire population.
<center>
```{r echo = FALSE, eval = TRUE, fig.width=3.5, fig.height=2.75}
pop$samp <- sample(0:1, 100, prob = c(0.9,0.10), replace = TRUE)
pop$samp <- as.factor(pop$samp)
ggplot(data = pop) + geom_point(mapping = aes(x = x, y = y, color = samp), size = 2, alpha = 0.8) + scale_color_manual(values = c("blue", "orange")) + guides(color = FALSE) + labs(title = "Simple Random Sample")
```
</center>

###

  * The *stratified sample* is used to ensure that we include representatives of all groups within our sample. Stratified sampling is particularly useful in cases where the population is segmented (that is, there are clear groups which may potentially have different responses)
  <center>![](images/Stratified.png)</center>

###

  * The *cluster sample* is used when we can argue that there are many small "populations" that are truly representative of the larger population. The clustering method is typically used to reduce costs (financial or otherwise). From the collection of *clusters*, a random sample is selected and as many observations as possible are collected from within each of those chosen clusters.
  
###

  * A variation on clustering is called *two-stage* or *multi-stage* sampling. With this method, observations are first clustered and clusters are chosen at random. Second, within each of the chosen clusters a simple random sample is taken. Notice that this method occurs in two stages, as the name suggests.
  <center> ![](images/cluster.png)</center>

###

  * The *convenience sample* is the most commonly used sampling method. Unfortunately, it is also the worst. When researchers sample from individuals they have "easy access" to, they are conducting a convenience sample. There are always hidden biases in these samples. Do a quick Google search for "FDR versus Alf Landon Sampling Error" to see a very famous example here. In addition, much of the error in predicting the results of the 2016 presidential election may be attributable to convenience sampling.

## Experimental Design

###

**Experiment versus Observational Study**: Beyond just sampling, there are multiple methods for collecting data. We can just *observe* what happens naturally (without manipulating any conditions) or we can run an *experiment*. In experiments we manipulate one or more conditions, utilizing a control and treatment group(s). The advantage to an experiment is that we can infer cause and effect relationships (this is extremely important in medical studies), but in observational studies we can only discuss an association between variables.

###

**Predictor versus Response Variables**: Typically in statistics we will identify a question (whose answer corresponds to a response variable) with respect to a population. We will take a representative sample of that population. From that sample we will collect observed responses as well as observations on other variables (maybe age, level of formal education, political affiliation, etc). 
The non-response variables are called *predictor variables*. In general, predictor variables are quantities which we expect *may be associated with* the response variable.

###

There's lots more to learn about experimental design, but it is beyond the scope of our course. You should read pages 32 through 35 of [OpenIntro Statistics, 4Ed](https://www.openintro.org/book/os/) as a starting point.

## Submit

```{r context="server"}
learnrhash::encoder_logic()
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(
  ui_before = shiny::div(
    "If you have completed this tutorial and are happy with all of your",
    "solutions, please click the button below to generate your hash and",
    "submit it using the the corresponding BrightSpace Quiz:",
    shiny::tags$br()
  ), 
  ui_after = shiny::tags$a(href = "https://learn.snhu.edu", "BrightSpace")
)
```

## Summary

**Summary**: Here's a quick summary of the most important ideas from this first notebook.

  * Data is stored in a table called a data frame. The rows of the data frame are observations and the columns are collected variables.
  * Data is either numerical or categorical -- to determine type, ask "is the average of these observations meaningful?"
  * Two variables are associated if a change in one has predictive value about a change in the other.
  * There are many ways data can be collected, but in order to produce meaningful results we must use random sampling.
  * Results from a sample can be generalized only to a population for which that sample is representative.
